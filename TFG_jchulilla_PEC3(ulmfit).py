# -*- coding: utf-8 -*-
"""Copia de TFG_jchulilla_PEC2(ulmfit).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JvGt73TQBy3uj3GHWzIM6u9QXOMQvxxg
"""

import sys
import numpy as np
from time import time
import pandas as pd
import warnings
from sklearn.utils import shuffle # to shuffle the data    

import re
!pip install contractions
import contractions

import fastai
from fastai.text import *
from fastai.callbacks import *

#######################################
### DATA READ                       ###
#######################################

def read_data(fileName,rows_to_process):
    """Loads and prepares the data in fileName.
    - Columns:
        text: the review of the movie 
        label : the sentiment label of the movie review
    """
    # Load CSV file
    sampleData=pd.read_csv(fileName,header=0)#,error_bad_lines=False)
    sampleData = shuffle(sampleData, random_state=123)
    sampleData = sampleData.iloc[0:rows_to_process,:]

    # Return the standardized data and the truth
    return sampleData

###########################
### Preprocessing Data  ###
###########################

def preprocess_text(text):
    """Basic cleaning of texts."""
    # Remove http links
    text=re.sub(r'http\S+',' ', str(text))

    # remove html markup
    text=re.sub('(<.*?>)',' ',str(text))

    # remove between square brackets
    text=re.sub('\[[^]]*\]', ' ', text)

    #remove non-ascii
    text=re.sub('[^\x00-\x7F]',' ',str(text))
    
    #remove hyphen not between characters
    text=re.sub('(-[^a-zA-Z0-9])',' ',str(text))

    #remove whitespace
    text=text.strip()

    #lowercase
    for f in re.findall("([A-Z]+)", text):
        text = text.replace(f, f.lower())
    
    #Replace contractions
    text= contractions.fix(str(text)) 

    return text

#################################################
#### Reading and preprocessing             ######
#################################################
path='/content/drive/My Drive/TFG'

#Supress warning messages on output
warnings.filterwarnings('ignore')
ROWS_TO_PROCESS = 1000000  #FOR TESTING PURPOSES!!!!!!!!!!!!
PERCENTAGE_TO_PROCESS = 80   #FOR TESTING PURPOSES!!!!!!!!!!!!
ROWS_TO_PROCESS_TRAIN = int(ROWS_TO_PROCESS*(PERCENTAGE_TO_PROCESS/100))
ROWS_TO_PROCESS_TEST = int(ROWS_TO_PROCESS*(100-PERCENTAGE_TO_PROCESS/100))

#Initial time mark
time_ini = time()
print('Reading .....')

# Loading train data
DataTrain=read_data(path+'/Train.csv',ROWS_TO_PROCESS_TRAIN)
    
# Loading test data
DataTest=read_data(path+'/Test.csv',ROWS_TO_PROCESS_TEST)

print(DataTrain.describe())

#Preprocessing text
print('Preprocessing text.....')
DataTrain['text']=DataTrain['text'].apply(preprocess_text)
DataTest['text']=DataTest['text'].apply(preprocess_text)

print("Total time....","{0:.{1}f}".format(time()- time_ini,2),"secs")

#############################
#### Ingest LM           ####
#############################
path='/content/drive/My Drive/TFG'

#Initial time mark
time_ini = time()
print('Ingesting LM.....')

#Ingest LM
data_lm = TextLMDataBunch.from_df('.', DataTrain,DataTest,text_cols='text',label_cols='label')
#data_lm.save(path+'/data_lm.pkl')
print(data_lm.show_batch())

print("Total time....","{0:.{1}f}".format(time()- time_ini,2),"secs")

#############################
#### Ingest Classifier   ####
#############################
path='/content/drive/My Drive/TFG'

#Initial time mark
time_ini = time()
print('Ingesting Classifier.....')

#Ingest Classifier
data_clas  = TextClasDataBunch.from_df('.', train_df=DataTrain,text_cols='text',label_cols='label',valid_df=DataTest,vocab=data_lm.train_ds.vocab)
#data_clas.save(path+'/data_clas.pkl')
print(data_clas.show_batch())

print("Total time....","{0:.{1}f}".format(time()- time_ini,2),"secs")

#############################
#### Language Model      ####
#############################
path='/content/drive/My Drive/TFG'

#Using pretrained LM
data_lm = load_data(path, 'data_lm.pkl') #FOR TESTING PURPOSES!!!!!!!!!!!!
learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3,pretrained=True)
#learn.freeze()

#finding learning rate
learn.lr_find()
print(learn.recorder.plot(suggestion=True))
best_lf_lr = learn.recorder.min_grad_lr
best_lf_lr

#Modelling 1 epoch
learn.fit_one_cycle(1,slice(best_lf_lr/(2.6**4),best_lf_lr),callbacks=[SaveModelCallback(learn, name=path+"/lm1")],moms=(0.8,0.7))
learn.save_encoder(path+'/encoder1')

### All layers unfrozen for LM
path='/content/drive/My Drive/TFG'

learn.unfreeze()

#finding learning rate
learn.lr_find()
print(learn.recorder.plot(suggestion=True))
best_lf_lr = learn.recorder.min_grad_lr
best_lf_lr

#Modelling 1 epoch
learn.fit_one_cycle(1,slice(best_lf_lr/(2.6**4),best_lf_lr),callbacks=[SaveModelCallback(learn, name=path+"/best_lm")],moms=(0.8,0.7))
learn.save_encoder(path+'/best_enc')

#########################
#### Classifier      ####
#########################
path='/content/drive/My Drive/TFG'

#Using pretrained model for sentiment analysis
data_clas = load_data(path, 'data_clas.pkl') #FOR TESTING PURPOSES!!!!!!!!!!!!
learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3)
learn.load_encoder(path+'/best_enc')

#finding learning rate
learn.lr_find()
print(learn.recorder.plot(suggestion=True))
best_lf_lr = learn.recorder.min_grad_lr
best_lf_lr

#Modelling 1 epoch & 1 layer unfrozen
learn.freeze_to(-1)
learn.fit_one_cycle(1, best_lf_lr,callbacks=[SaveModelCallback(learn, name=path+"/clas1")],moms=(0.8,0.7))
#learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2),callbacks=[SaveModelCallback(learn, name=path+"/clas1")],moms=(0.8,0.7))

### 2 layers unfrozen for Classification
learn2=learn.load(path+'/clas1')

#finding learning rate
learn2.lr_find()
print(learn2.recorder.plot(suggestion=True))
best_lf_lr = learn2.recorder.min_grad_lr
best_lf_lr

#Modelling 3 epoch & 2 layer unfrozen
learn2.freeze_to(-2)
learn2.fit_one_cycle(1, best_lf_lr,callbacks=[SaveModelCallback(learn2, name=path+"/clas2")],moms=(0.8,0.7))
#learn2.fit_one_cycle(1, slice(1e-6/(2.6**4),1e-6),callbacks=[SaveModelCallback(learn2, name=path+"/clas2")],moms=(0.8,0.7))

### 3 layers unfrozen for Classification
learn3=learn2.load(path+'/clas2')

#finding learning rate
learn3.lr_find()
print(learn3.recorder.plot(suggestion=True))
best_lf_lr = learn3.recorder.min_grad_lr
best_lf_lr

#Modelling 1 epoch & 3 layer unfrozen
learn3.freeze_to(-3)
#learn3.fit_one_cycle(3, best_lf_lr,callbacks=[SaveModelCallback(learn3, name=path+"/clas3")],moms=(0.8,0.7))
learn3.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3),callbacks=[SaveModelCallback(learn3, name=path+"/clas3")],moms=(0.8,0.7))

### All layers unfrozen for Classification
learn4=learn3.load(path+'/clas3')

#finding learning rate
learn4.lr_find()
print(learn4.recorder.plot(suggestion=True))
best_lf_lr = learn4.recorder.min_grad_lr
best_lf_lr

#Modelling 1 epoch & all layers unfrozen
learn4.unfreeze()
#learn4.fit_one_cycle(1, best_lf_lr,callbacks=[SaveModelCallback(learn4, name=path+"/best_clas")],moms=(0.8,0.7))
learn4.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-3),callbacks=[SaveModelCallback(learn4, name=path+"/best_clas")], moms=(0.9,0.8))

#Saving model
path='/content/drive/My Drive/TFG'
#learn4.export(path+'/learn.pkl')

#Loading model
path='/content/drive/My Drive/TFG'
learn4 = load_learner(path,'learn.pkl')

# get predictions
preds, targets = learn4.get_preds()

predictions = np.argmax(preds, axis = 1)
pd.crosstab(predictions, targets)

#Testing examples
print(learn4.predict("huge film"))

print(learn4.predict("so bad but i like it"))

print(learn4.predict("Last but not the least; the androgynous beauty of the sexy Ren√©e Soutendijk "+
 "perfectly fits to her role of a woman that attracts a gay writer. My vote is eight."))

print(learn4.predict("This film takes you on one family's impossible journey, and makes you feel " +
 "every step of their odyssey. Beautifully acted and photographed, heartbreakingly real. Its last "+
 "line, with its wistful hope, is one of the more powerful in memory"))

print(learn4.predict("I saw this in the summer of 1990. I'm still annoyed by how bad this movie is "+
 "in 2001.<br /><br />Implausible plot. You'd have to be a child to think this could happen."+
 "<br /><br />I'm just really annoyed by it. Don't see this."))

print(learn4.predict("Nothing great here but a nicely acted story about an abused deaf wife (Fonda) "+
 "of a small time crook (Bochner)who gets involved with one of her husband's plans and his mistress. "+
 "Sutherland and Weber are cops drawn into what turns out to be a unmysterious murder investigation "+
 "and the story just flows along."))